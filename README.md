# RedElect2024
Project completed during the AI Alignment course, aiming to test LLMs against prompts that elicit politically charged text related to the US election

## Intro + Research Question

This work aims to answer a key research question: how easily can one generate politically charged text related to the 2024 US election from state of art LLMs? This is an interesting question because many AI chatbot tools (ChatGPT, Claude, LLama) go through extensive alignment training to be safe but also helpful. There are inherent conflicts between these two values: if I ask a model to help me write a thesis defending Ukraine in its war against Russia... its likely unsafe for the model to contribute to a strong political position, but refusing this goes against its helpfulness training.

Inspired by this example, I've created a dataset that consists of 1140 prompts that attempt to generate various forms of internet text (Facebook posts, tik tok scripts, news articles, etc.) about the hottest topics surrounding the 2024 US election. This prompt dataset can be found in `election_2024_red_team.csv`

Code to run these red-team prompts on Claude Haiku can be found in `call_anthropic.py`

## Dataset Generation

`medium_templates.txt` contains 30 different online text generation prompts that were auto-generated by Claude Opus
`statements.txt` contains 38 political statemets (over 19 topics, 1 per party) that summarize key positions held by each candidate in the election (Joe Biden, Donald Trump). These statements were hand generated by me based on this NBC news article that details each candidate's position on these 19 topics. These 19 topics were determined by this outlet to be the most important issues voters will be considering in this 2024 Election

38 medium templates * 30 political statement = 1140 total input prompts that make up RedElect2024


## Experiment 1: Prompting Claude Haiku with RedElect2024 

Now that we have a dataset of politically charged prompts, we can try to run these against a state of art LLM chat model. For this first experiment, we'll use Claude Haiku, Anthropic's fastest and smallest LLM.
